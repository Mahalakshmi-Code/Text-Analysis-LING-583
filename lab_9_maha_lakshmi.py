# -*- coding: utf-8 -*-
"""lab_9 Maha Lakshmi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13t1aX0v7TyqzJEYTSmb-Z_ltVCHXxvSz

# LAB 9: Feed-forward networks

**Objectives:**
- experiment with various simple neural architectures
"""

import numpy as np
import pandas as pd
from tqdm import tqdm
import torchtext

tqdm.pandas()

"""Install some packages that aren't part of standard colab"""

!pip install -q portalocker cytoolz skorch transformers

"""---

## Load data
"""

from torchtext.datasets import YelpReviewPolarity
from sklearn.utils import resample

def make_df(examples):
  rows = [{'label':label, 'text':text} for (label,text) in examples]
  return pd.DataFrame(rows)

train = make_df(YelpReviewPolarity(split='train'))
test = make_df(YelpReviewPolarity(split='test'))

train = resample(train, replace=False, n_samples=300000, stratify=train["label"], random_state=1)

import spacy

nlp = spacy.load(
    "en_core_web_sm",
    exclude=["tagger", "parser", "ner", "lemmatizer", "attribute_ruler"],
)


def tokenize(text):
    doc = nlp.tokenizer(text)
    return [t.norm_ for t in doc if not (t.is_space or t.is_punct or t.like_num)]

train["tokens"] = train["text"].progress_apply(tokenize)
test["tokens"] = test["text"].progress_apply(tokenize)

"""---

## Scikit-learn 

Try using scikit-learn's SGDClassifier. Skorch sets aside 20% of the training data for validation, so to make things more directly comparable we'll do the same here.
"""

from cytoolz import identity
from sklearn.linear_model import SGDClassifier
from sklearn.metrics import accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from scipy.stats.distributions import loguniform, uniform

vectorizer = CountVectorizer(analyzer=identity)
sgd_train, _ = train_test_split(train, test_size=0.2)
X_train = vectorizer.fit_transform(sgd_train['tokens'])
X_test =  vectorizer.transform(test['tokens'])

y_train = sgd_train["label"]
y_test = test["label"]

sgd = SGDClassifier()
sgd.fit(X_train, y_train)
sgd_predicted = sgd.predict(X_test)
print(accuracy_score(y_test, sgd_predicted))

search = GridSearchCV(sgd, param_grid={'alpha':[1e-6,1e-4,1e-2,1]}, n_jobs=-1)
search.fit(X_train, y_train)
search.best_params_

sgd.set_params(**search.best_params_)
sgd.fit(X_train, y_train)
sgd_predicted = sgd.predict(X_test)
print(accuracy_score(y_test, sgd_predicted))

"""----

## Data prepartion

The input to scikit-learn classifiers has always been a document-term matrix (usually created by `CountVectorizer`). The input to a torch classifier should be a list of token ids. This will give us more flexibility in how the classifier represents the text. 

But, there's a complication: both torch and sklearn expect all document vectors to be the same length. That's easy for a document-term matrix, since the dimensionality of each document vector is the vocabulary size. If we're representing individual tokens, though, the length of the document vectors is equal to the length of the document, which won't be the same from doc to doc. 

To make all the doc lengths equal, we can add dummy words (`<pad>`) to end the of short texts. This will make all the docs as long as the longest doc in the dataset. Given the way doc lengths are distributed, though, very long texts are unusual all we'll be doing a lot of extra padding. 

A compromise is to cut off very long docs and pad out short ones so they all docs end up being of medium length.
"""

import torch
import torchtext
import torch.nn.functional as F
from torch import nn, optim
from torchtext.transforms import VocabTransform, ToTensor, Truncate

"""Doc lengths range up to about 1,000 words but most are < 300 words."""

train['tokens'].apply(len).plot(kind='hist')

vocab = torchtext.vocab.build_vocab_from_iterator(train['tokens'], specials=['<pad>','<unk>'], min_freq=2)# Changed min freq to 2
vocab.set_default_index(vocab.get_stoi()['<unk>'])

PAD_IDX = vocab.get_stoi()['<pad>']

with torch.no_grad():
  tr = nn.Sequential(VocabTransform(vocab), Truncate(300), ToTensor(padding_value=PAD_IDX))

  X_train = tr(list(train['tokens'])).to('cuda')
  X_test = tr(list(test['tokens'])).to('cuda')

  y_train = train["label"].to_numpy(dtype=np.float32) - 1
  y_test = test["label"].to_numpy(dtype=np.float32) - 1

"""---

## Embedding + one layer

This model is structurally a lot like `TruncatedSVD` + `SGDClassifier`. The big difference, though, is that it's trained as a single unit so that the dimensionality reduction step optimizies classification accuracy.
"""

from skorch import NeuralNetBinaryClassifier

"""Define a simple 'bag of embeddings' model

$$
\mathbf{x}=\textrm{mean}\left(\mathbf{e}(w_1),\mathbf{e}(w_2),\ldots,\mathbf{e}(w_n)\right)$$
$$\mathbf{y}=\sigma\left(\mathbf{Wx}\right)$$

Some things to try:
- change size of embedding (`embedding_size`) and pooling mode (`pooling`). See documentation at https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#embeddingbag
- change `optimizer__weight_decay` (similar to to `alpha` for `SGDClassifier`)
- change `min_freq` in vocabulary
"""

class EmbeddingLogisticR(nn.Module):
    def __init__(self, vocab_size, embedding_size, pooling, padding_idx):
        super().__init__()

        self.embedding = nn.EmbeddingBag(vocab_size, embedding_size, padding_idx=padding_idx, mode=pooling)
        self.output = nn.Linear(embedding_size, 1)

    def forward(self, X, **kwargs):
        e = self.embedding(X)
        y = self.output(e)
        return y

net = NeuralNetBinaryClassifier(
    EmbeddingLogisticR,
    module__vocab_size = len(vocab),
    module__embedding_size = 50, # changed module__embedding_size 
    module__pooling = "mean",
    module__padding_idx = PAD_IDX,
    max_epochs = 10,
    verbose = 1,
    device = "cuda",
    optimizer = optim.Adam,
    batch_size = 1024,
    optimizer__weight_decay = 0.0001# Changed optimizer__weight_decay value
)

net.fit(X_train, y_train)
net_predicted = net.predict(X_test)
print(accuracy_score(y_test, net_predicted))
#0.911 is

"""**Tried different values for module__embedding_size,optimizer__weight_decay. Changed minimum frequency to 2 in vocabulary.
  The accuracy score is not getting more than 0.911 which we got in class**




"""

net.set_params(verbose=0, max_epochs=5)
search = GridSearchCV(net, 
                      param_grid={'optimizer__weight_decay':[1e-6,1e-4,1e-2,1]}, 
                      verbose=2, refit=False, cv=2)
search.fit(X_train, y_train)
search.best_params_

net.set_params(verbose=1, **search.best_params_)
net.fit(X_train, y_train)
net_predicted = net.predict(X_test)
print(accuracy_score(y_test, net_predicted))

"""---

## Deeper learning

Next, we'll add more hidden layers and apply dropout. The model equations are

$$\mathbf{x}=\textrm{mean}\left(\mathbf{e}(w_1),\mathbf{e}(w_2),\ldots,\mathbf{e}(w_n)\right)$$
$$\mathbf{h}_1=\sigma(\mathbf{Wx})$$
$$\mathbf{h}_2=\sigma(\mathbf{U}_2\mathbf{h}_1)$$
$$\mathbf{y}=\sigma\left(\mathbf{h}_2\right)$$

Additional things to try:
- Change the non-linearity used in the hidden layers `non_lin` (tanh, relu, etc). See documentation at https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions
- Change size of hidden layers `hidden_units`
- Add additional hidden layers
- Change dropout probability `dropout`

"""

class EmbeddingMultilayer(nn.Module):
    def __init__(self, vocab_size, embedding_size, pooling, padding_idx, non_lin, hidden_units, dropout):
        super().__init__()

        self.non_lin = non_lin
        self.embedding = nn.EmbeddingBag(vocab_size, embedding_size, padding_idx=padding_idx, mode=pooling)
        self.hidden1 = nn.Linear(embedding_size, hidden_units)
        self.hidden2 = nn.Linear(hidden_units, hidden_units)
        self.output = nn.Linear(hidden_units, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, X, **kwargs):
        e = self.embedding(X)
        e = self.dropout(e)
        h = self.non_lin(self.hidden1(e))
        h = self.dropout(h)
        h = self.non_lin(self.hidden2(h))
        h = self.dropout(h)
        y = self.output(h)
        return y

net = NeuralNetBinaryClassifier(
    EmbeddingMultilayer,
    module__vocab_size = len(vocab),
    module__embedding_size = 32,
    module__pooling = "mean",
    module__padding_idx = PAD_IDX,
    module__non_lin = F.sigmoid,
    module__hidden_units = 32,
    module__dropout = 0.7,
    max_epochs = 10,
    verbose = 1,
    device = "cuda",
    batch_size = 1024,
    optimizer = optim.Adam,
    optimizer__weight_decay = 0,
    iterator_train__shuffle = True
)

net.fit(X_train, y_train)
net_predicted = net.predict(X_test)
print(accuracy_score(y_test, net_predicted))

"""---

## Pre-trained vectors

This model is the same as the one above, except it uses pre-trained GloVe vectors

Things to try:
- Different GloVe models (6B, twitter.27B, 42B, 840B) and dimensions; see https://nlp.stanford.edu/projects/glove/
- Change setting of `freeze` to allow fine-tuning
"""

from torchtext.vocab import GloVe

glove = GloVe(name='twitter.27B', dim=100)# trying with 42B model

vectors = torch.vstack([glove.get_vecs_by_tokens([vocab.lookup_token(i)]) for i in range(len(vocab))])

class GloveMultilayer(nn.Module):
    def __init__(self, vocab_size, vectors, freeze, pooling, padding_idx, non_lin, hidden_units, dropout):
        super().__init__()

        self.non_lin = non_lin
        self.embedding = nn.EmbeddingBag.from_pretrained(vectors, padding_idx=padding_idx, 
                                                         freeze=freeze, mode=pooling)
        self.hidden1 = nn.Linear(vectors.shape[1], hidden_units)
        self.hidden2 = nn.Linear(hidden_units, hidden_units)
        self.output = nn.Linear(hidden_units, 1)
        self.dropout = nn.Dropout(dropout)

    def forward(self, X, **kwargs):
        e = self.embedding(X)
        e = self.dropout(e)
        h = self.non_lin(self.hidden1(e))
        h = self.dropout(h)
        h = self.non_lin(self.hidden2(h))
        h = self.dropout(h)
        y = self.output(h)
        return y

net = NeuralNetBinaryClassifier(
    GloveMultilayer,
    module__vocab_size = len(vocab),
    module__vectors = vectors,
    module__freeze = True,
    module__pooling = "mean",
    module__padding_idx = PAD_IDX,
    module__non_lin = F.sigmoid,
    module__hidden_units = 32,
    module__dropout = 0.7,
    max_epochs = 10,
    verbose = 1,
    device = "cuda",
    batch_size = 1024,
    optimizer = optim.Adam,
    optimizer__weight_decay = 0,
    iterator_train__shuffle = True
)

net.fit(X_train, y_train)
net_predicted = net.predict(X_test)
print(accuracy_score(y_test, net_predicted))

"""**With 6B the accuracy score is 0.7672. And with 42B the accuracy score increased to 0.8169**

# **Trying with Cnn**
"""

class CNNEncoder(nn.Module):
    def __init__(self, vocab_size, embedding_size, kernel_sizes, num_filters, padding_idx, dropout):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)
        self.convs = nn.ModuleList([nn.Conv1d(embedding_size, num_filters, kernel_size) for kernel_size in kernel_sizes])
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        x = self.embedding(x).transpose(1, 2)  # (batch_size, embedding_size, seq_len)
        x = [F.relu(conv(x)) for conv in self.convs]  # [(batch_size, num_filters, ?), ...]
        x = [F.max_pool1d(conv_out, conv_out.shape[2]).squeeze(2) for conv_out in x]  # [(batch_size, num_filters), ...]
        x = torch.cat(x, dim=1)  # (batch_size, num_filters * len(kernel_sizes))
        x = self.dropout(x)
        return x


class CNNEncoderMultilayer(nn.Module):
    def __init__(self, vocab_size, embedding_size, kernel_sizes, num_filters, padding_idx, non_lin, hidden_units, dropout):
        super().__init__()

        self.encoder = CNNEncoder(vocab_size, embedding_size, kernel_sizes, num_filters, padding_idx, dropout)
        self.hidden1 = nn.Linear(num_filters * len(kernel_sizes), hidden_units)
        self.hidden2 = nn.Linear(hidden_units, hidden_units)
        self.output = nn.Linear(hidden_units, 1)
        self.dropout = nn.Dropout(dropout)
        self.non_lin = non_lin

    def forward(self, X, **kwargs):
        e = self.encoder(X)
        h = self.non_lin(self.hidden1(e))
        h = self.dropout(h)
        h = self.non_lin(self.hidden2(h))
        h = self.dropout(h)
        y = self.output(h)
        return y

net2 = NeuralNetBinaryClassifier(
    CNNEncoderMultilayer,
    module__vocab_size=len(vocab),
    module__embedding_size=50,
    module__kernel_sizes=[3, 4, 7],
    module__num_filters=200,
    module__padding_idx=PAD_IDX,
    module__non_lin=F.relu,
    module__hidden_units=64,
    module__dropout=0.7,
    max_epochs=10,
    verbose=1,
    device="cuda",
    batch_size=1024,
    optimizer=optim.Adam,
    optimizer__weight_decay=0,
    iterator_train__shuffle=True
)

net2.fit(X_train, y_train)
net2_predicted = net2.predict(X_test)
print(accuracy_score(y_test, net2_predicted))

"""****"""